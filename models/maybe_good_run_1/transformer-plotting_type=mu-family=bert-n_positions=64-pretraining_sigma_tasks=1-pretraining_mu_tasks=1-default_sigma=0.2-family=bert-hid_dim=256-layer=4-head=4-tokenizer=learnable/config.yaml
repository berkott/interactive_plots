model:
  attention_dropout: 0.0
  family: bert
  hidden_dropout: 0.0
  mlp: []
  n_embd: 256
  n_head: 4
  n_layer: 4
  n_positions: 64
  position_embedding: true
  tokenizer: learnable
  use_layer_norm: null
model_type: transformer
out_dir: models/transformer-plotting_type=mu-family=bert-n_positions=64-pretraining_sigma_tasks=1-pretraining_mu_tasks=1-default_sigma=0.2-family=bert-hid_dim=256-layer=4-head=4-tokenizer=learnable
training:
  batch_size: 4
  curriculum:
    dims:
      end: 1
      inc: 0
      interval: 100001
      start: 1
    points:
      end: 64
      inc: 0
      interval: 10000
      start: 64
  data_kwargs: null
  default_sigma: 0.2
  learning_rate: 0.0001
  loss: normalized_mean_squared_error
  plotting_type: mu
  pretraining_mu_tasks: 1
  pretraining_sigma_tasks: 1
  save_every_steps: 444333222111
  save_plot_every_steps: 100
  save_verbose_plot_every_steps: 10000
  train_steps: 100001
wandb:
  entity: chsanford4
  log_every_steps: 50
  name: normal_means
  project: in-context-model-selection
