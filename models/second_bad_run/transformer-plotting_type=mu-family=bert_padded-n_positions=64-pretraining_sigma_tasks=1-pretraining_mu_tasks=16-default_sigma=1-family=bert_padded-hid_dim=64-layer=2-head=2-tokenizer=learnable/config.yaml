model:
  attention_dropout: 0.0
  family: bert_padded
  hidden_dropout: 0.0
  mlp: []
  n_embd: 64
  n_head: 2
  n_layer: 2
  n_positions: 64
  tokenizer: learnable
  use_layer_norm: null
model_type: transformer
out_dir: models/transformer-plotting_type=mu-family=bert_padded-n_positions=64-pretraining_sigma_tasks=1-pretraining_mu_tasks=16-default_sigma=1-family=bert_padded-hid_dim=64-layer=2-head=2-tokenizer=learnable
training:
  batch_size: 4
  curriculum:
    dims:
      end: 1
      inc: 0
      interval: 100001
      start: 1
    points:
      end: 64
      inc: 0
      interval: 10000
      start: 64
  data_kwargs: null
  default_sigma: 1
  learning_rate: 0.0001
  loss: normalized_mean_squared_error
  plotting_type: mu
  pretraining_mu_tasks: 16
  pretraining_sigma_tasks: 1
  save_every_steps: 100000
  save_plot_every_steps: 200
  save_verbose_plot_every_steps: 10000
  train_steps: 100001
wandb:
  entity: chsanford4
  log_every_steps: 50
  name: normal_means
  project: in-context-model-selection
